{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Face Recognition with OpenCV\n",
        "\n",
        "First, clone this [repository](https://github.com/ardhiraka/ftds_face_recog). Add dataset folder and images folder, fill with your photos.\n",
        "\n",
        "```dir\n",
        ".\n",
        "├── dataset\n",
        "│   ├── raka [6 images]\n",
        "│   ├── anggie [6 images]\n",
        "│   └── unknown [6 images]\n",
        "├── images\n",
        "│   ├── raka.jpg\n",
        "│   ├── unknown.jpg\n",
        "│   └── anggie_raka.jpg\n",
        "├── face_detection_model\n",
        "│   ├── deploy.prototxt\n",
        "│   └── res10_300x300_ssd_iter_140000.caffemodel\n",
        "├── output\n",
        "│   ├── embeddings.pickle\n",
        "│   ├── le.pickle\n",
        "│   └── recognizer.pickle\n",
        "├── extract_embeddings.py\n",
        "├── openface.nn4.small2.v1.t7\n",
        "├── train_model.py\n",
        "├── recognize.py\n",
        "└── recognize_video.py\n",
        "```\n",
        "\n",
        "This project has four directories in the root folder:\n",
        "\n",
        "- dataset/ : Contains our face images organized into subfolders by name.\n",
        "- images/ : Contains three test images that we’ll use to verify the operation of our model.\n",
        "- face_recognition_model/ : Contains a pre-trained Caffe deep learning model provided by OpenCV to detect faces. This model detects and localizes faces in an image.\n",
        "- output/ : Contains my output pickle files. If you’re working with your own dataset, you can store your output files here as well. The output files include:\n",
        "- embeddings.pickle : A serialized facial embeddings file. Embeddings have been computed for every face in the dataset and are stored in this file.\n",
        "- le.pickle : Our label encoder. Contains the name labels for the people that our model can recognize.\n",
        "- recognizer.pickle : Our Linear Support Vector Machine (SVM) model. This is a machine learning model rather than a deep learning model and it is responsible for actually recognizing faces.\n",
        "\n",
        "Let’s summarize the five files in the root directory:\n",
        "\n",
        "- extract_embeddings.py : We’ll review this file in Step #1 which is responsible for using a deep learning feature extractor to generate a 128-D vector describing a face. All faces in our dataset will be passed through the neural network to generate embeddings.\n",
        "- openface_nn4.small2.v1.t7 : A Torch deep learning model which produces the 128-D facial embeddings. We’ll be using this deep learning model in Steps #1, #2, and #3.\n",
        "- train_model.py : Our Linear SVM model will be trained by this script in Step #2. We’ll detect faces, extract embeddings, and fit our SVM model to the embeddings data.\n",
        "- recognize.py : In Step #3 and we’ll recognize faces in images. We’ll detect faces, extract embeddings, and query our SVM model to determine who is in an image. We’ll draw boxes around faces and annotate each box with a name.\n",
        "- recognize_video.py : How to recognize who is in frames of a video stream just as we did in Step #3 on static images.\n"
      ],
      "metadata": {
        "id": "8cm6QPa8LUvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step #1: Extract embeddings from face dataset\n",
        "\n",
        "Now that we understand how face recognition works and reviewed our project structure, let’s get started building our OpenCV face recognition pipeline.\n",
        "\n",
        "Before that, we have to install some library:\n",
        "\n",
        "```shell\n",
        "pip install opencv-python\n",
        "\n",
        "pip install --upgrade imutils\n",
        "```\n",
        "\n",
        "Open up the `extract_embeddings.py` file and insert the following code:\n"
      ],
      "metadata": {
        "id": "6rkevIfRLUva"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# import the necessary packages\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import pickle\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# construct the argument parser and parse the arguments\n",
        "ap = argparse.ArgumentParser()\n",
        "ap.add_argument(\"-i\", \"--dataset\", required=True,\n",
        "\thelp=\"path to input directory of faces + images\")\n",
        "ap.add_argument(\"-e\", \"--embeddings\", required=True,\n",
        "\thelp=\"path to output serialized db of facial embeddings\")\n",
        "ap.add_argument(\"-d\", \"--detector\", required=True,\n",
        "\thelp=\"path to OpenCV's deep learning face detector\")\n",
        "ap.add_argument(\"-m\", \"--embedding-model\", required=True,\n",
        "\thelp=\"path to OpenCV's deep learning face embedding model\")\n",
        "ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n",
        "\thelp=\"minimum probability to filter weak detections\")\n",
        "args = vars(ap.parse_args())"
      ],
      "outputs": [],
      "metadata": {
        "id": "NUvT5g5DLUvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we process our command line arguments:\n",
        "\n",
        "- --dataset : The path to our input dataset of face images.\n",
        "- --embeddings : The path to our output embeddings file. Our script will compute face embeddings which we’ll serialize to disk.\n",
        "- --detector : Path to OpenCV’s Caffe-based deep learning face detector used to actually localize the faces in the images.\n",
        "- --embedding-model : Path to the OpenCV deep learning Torch embedding model. This model will allow us to extract a 128-D facial embedding vector.\n",
        "- --confidence : Optional threshold for filtering week face detections.\n",
        "\n",
        "Now that we’ve imported our packages and parsed command line arguments, lets load the face detector and embedder from disk:\n"
      ],
      "metadata": {
        "id": "EqY5MditLUvf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# load our serialized face detector from disk\n",
        "print(\"[INFO] loading face detector...\")\n",
        "protoPath = os.path.sep.join([args[\"detector\"], \"deploy.prototxt\"])\n",
        "modelPath = os.path.sep.join([args[\"detector\"],\n",
        "\t\"res10_300x300_ssd_iter_140000.caffemodel\"])\n",
        "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
        "# load our serialized face embedding model from disk\n",
        "print(\"[INFO] loading face recognizer...\")\n",
        "embedder = cv2.dnn.readNetFromTorch(args[\"embedding_model\"])"
      ],
      "outputs": [],
      "metadata": {
        "id": "q4Qz_w8DLUvg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we load the face detector and embedder:\n",
        "\n",
        "- detector : Loaded via Lines 3-6. We’re using a Caffe based DL face detector to localize faces in an image.\n",
        "- embedder : Loaded on Line 9. This model is Torch-based and is responsible for extracting facial embeddings via deep learning feature extraction.\n",
        "\n",
        "Notice that we’re using the respective cv2.dnn functions to load the two separate models.\n",
        "\n",
        "Moving forward, let’s grab our image paths and perform initializations:\n"
      ],
      "metadata": {
        "id": "iROr56MtLUvh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# grab the paths to the input images in our dataset\n",
        "print(\"[INFO] quantifying faces...\")\n",
        "imagePaths = list(paths.list_images(args[\"dataset\"]))\n",
        "# initialize our lists of extracted facial embeddings and\n",
        "# corresponding people names\n",
        "knownEmbeddings = []\n",
        "knownNames = []\n",
        "# initialize the total number of faces processed\n",
        "total = 0"
      ],
      "outputs": [],
      "metadata": {
        "id": "AVjLL8q2LUvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The imagePaths list, built on Line 3, contains the path to each image in the dataset. I’ve made this easy via my imutils function, paths.list_images .\n",
        "\n",
        "Our embeddings and corresponding names will be held in two lists: knownEmbeddings and knownNames (Lines 6 and 7).\n",
        "\n",
        "We’ll also be keeping track of how many faces we’ve processed via a variable called total (Line 9).\n",
        "\n",
        "Let’s begin looping over the image paths — this loop will be responsible for extracting embeddings from faces found in each image:\n"
      ],
      "metadata": {
        "id": "jwrHA2q6LUvi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# loop over the image paths\n",
        "for (i, imagePath) in enumerate(imagePaths):\n",
        "\t# extract the person name from the image path\n",
        "\tprint(\"[INFO] processing image {}/{}\".format(i + 1,\n",
        "\t\tlen(imagePaths)))\n",
        "\tname = imagePath.split(os.path.sep)[-2]\n",
        "\t# load the image, resize it to have a width of 600 pixels (while\n",
        "\t# maintaining the aspect ratio), and then grab the image\n",
        "\t# dimensions\n",
        "\timage = cv2.imread(imagePath)\n",
        "\timage = imutils.resize(image, width=600)\n",
        "\t(h, w) = image.shape[:2]"
      ],
      "outputs": [],
      "metadata": {
        "id": "JTfwPF9ILUvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We begin looping over imagePaths on Line 2.\n",
        "\n",
        "First, we extract the name of the person from the path (Line 6). To explain how this works, consider the following example in my Python shell:\n",
        "\n",
        "```shell\n",
        "$ python\n",
        ">>> from imutils import paths\n",
        ">>> import os\n",
        ">>> imagePaths = list(paths.list_images(\"dataset\"))\n",
        ">>> imagePath = imagePaths[0]\n",
        ">>> imagePath\n",
        "'dataset/raka/00004.jpg'\n",
        ">>> imagePath.split(os.path.sep)\n",
        "['dataset', 'raka', '00004.jpg']\n",
        ">>> imagePath.split(os.path.sep)[-2]\n",
        "'raka'\n",
        ">>>\n",
        "```\n",
        "\n",
        "Notice how by using imagePath.split and providing the split character (the OS path separator — “/” on unix and “\\” on Windows), the function produces a list of folder/file names (strings) which walk down the directory tree. We grab the second-to-last index, the persons name , which in this case is `raka` .\n",
        "\n",
        "Finally, we wrap up the above code block by loading the image and resize it to a known width (Lines 10 and 11).\n",
        "\n",
        "Let’s detect and localize faces:\n"
      ],
      "metadata": {
        "id": "xF0wg-WCLUvk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\t# construct a blob from the image\n",
        "\timageBlob = cv2.dnn.blobFromImage(\n",
        "\t\tcv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
        "\t\t(104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
        "\t# apply OpenCV's deep learning-based face detector to localize\n",
        "\t# faces in the input image\n",
        "\tdetector.setInput(imageBlob)\n",
        "\tdetections = detector.forward()"
      ],
      "outputs": [],
      "metadata": {
        "id": "-SqRTNV8LUvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On Lines 2-4, we construct a blob.\n",
        "\n",
        "From there we detect faces in the image by passing the imageBlob through the detector network (Lines 7-8).\n",
        "\n",
        "Let’s process the detections :\n"
      ],
      "metadata": {
        "id": "occJHJQpLUvn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\t# ensure at least one face was found\n",
        "\tif len(detections) > 0:\n",
        "\t\t# we're making the assumption that each image has only ONE\n",
        "\t\t# face, so find the bounding box with the largest probability\n",
        "\t\ti = np.argmax(detections[0, 0, :, 2])\n",
        "\t\tconfidence = detections[0, 0, i, 2]\n",
        "\t\t# ensure that the detection with the largest probability also\n",
        "\t\t# means our minimum probability test (thus helping filter out\n",
        "\t\t# weak detections)\n",
        "\t\tif confidence > args[\"confidence\"]:\n",
        "\t\t\t# compute the (x, y)-coordinates of the bounding box for\n",
        "\t\t\t# the face\n",
        "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\t\t\t# extract the face ROI and grab the ROI dimensions\n",
        "\t\t\tface = image[startY:endY, startX:endX]\n",
        "\t\t\t(fH, fW) = face.shape[:2]\n",
        "\t\t\t# ensure the face width and height are sufficiently large\n",
        "\t\t\tif fW < 20 or fH < 20:\n",
        "\t\t\t\tcontinue"
      ],
      "outputs": [],
      "metadata": {
        "id": "4y5LZLTqLUvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The detections list contains probabilities and coordinates to localize faces in an image.\n",
        "\n",
        "Assuming we have at least one detection, we’ll proceed into the body of the if-statement (Line 2).\n",
        "\n",
        "We make the assumption that there is only one face in the image, so we extract the detection with the highest confidence and check to make sure that the confidence meets the minimum probability threshold used to filter out weak detections (Lines 5-10).\n",
        "\n",
        "Assuming we’ve met that threshold, we extract the face ROI and grab/check dimensions to make sure the face ROI is sufficiently large (Lines 13-20).\n",
        "\n",
        "From there, we’ll take advantage of our embedder CNN and extract the face embeddings:\n"
      ],
      "metadata": {
        "id": "b1ig8bv-LUvo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\t\t\t# construct a blob for the face ROI, then pass the blob\n",
        "\t\t\t# through our face embedding model to obtain the 128-d\n",
        "\t\t\t# quantification of the face\n",
        "\t\t\tfaceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
        "\t\t\t\t(96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
        "\t\t\tembedder.setInput(faceBlob)\n",
        "\t\t\tvec = embedder.forward()\n",
        "\t\t\t# add the name of the person + corresponding face\n",
        "\t\t\t# embedding to their respective lists\n",
        "\t\t\tknownNames.append(name)\n",
        "\t\t\tknownEmbeddings.append(vec.flatten())\n",
        "\t\t\ttotal += 1"
      ],
      "outputs": [],
      "metadata": {
        "id": "_KEWgTHmLUvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We construct another blob, this time from the face ROI (not the whole image as we did before) on Lines 4-5.\n",
        "\n",
        "Subsequently, we pass the faceBlob through the embedder CNN (Lines 6-7). This generates a 128-D vector (vec ) which describes the face. We’ll leverage this data to recognize new faces via machine learning.\n",
        "\n",
        "And then we simply add the name and embedding vec to knownNames and knownEmbeddings , respectively (Lines 10-11).\n",
        "\n",
        "We also can’t forget about the variable we set to track the total number of faces either — we go ahead and increment the value on Line 12.\n",
        "\n",
        "We continue this process of looping over images, detecting faces, and extracting face embeddings for each and every image in our dataset.\n",
        "\n",
        "All that’s left when the loop finishes is to dump the data to disk:\n"
      ],
      "metadata": {
        "id": "VM7xGDEbLUvp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# dump the facial embeddings + names to disk\n",
        "print(\"[INFO] serializing {} encodings...\".format(total))\n",
        "data = {\"embeddings\": knownEmbeddings, \"names\": knownNames}\n",
        "f = open(args[\"embeddings\"], \"wb\")\n",
        "f.write(pickle.dumps(data))\n",
        "f.close()"
      ],
      "outputs": [],
      "metadata": {
        "id": "e9dAoZyRLUvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We add the name and embedding data to a dictionary and then serialize the data in a pickle file on Lines 2-6.\n",
        "\n",
        "At this point we’re ready to extract embeddings by running our script.\n",
        "\n",
        "From there, open up a terminal and execute the following command to compute the face embeddings with OpenCV:\n",
        "\n",
        "```shell\n",
        "python extract_embeddings.py --dataset dataset --embeddings output/embeddings.pickle --detector face_recognition_model --embedding-model openface.nn4.small2.v1.t7\n",
        "```\n"
      ],
      "metadata": {
        "id": "rNuP2nrBLUvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step #2: Train face recognition model\n",
        "\n",
        "At this point we have extracted 128-d embeddings for each face — but how do we actually recognize a person based on these embeddings? The answer is that we need to train a “standard” machine learning model (such as an SVM, k-NN classifier, Random Forest, etc.) on top of the embeddings.\n",
        "\n",
        "Open up the `train_model.py` file and insert the following code:\n"
      ],
      "metadata": {
        "id": "I6yoOW2bLUvs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# import the necessary packages\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "import argparse\n",
        "import pickle\n",
        "# construct the argument parser and parse the arguments\n",
        "ap = argparse.ArgumentParser()\n",
        "ap.add_argument(\"-e\", \"--embeddings\", required=True,\n",
        "\thelp=\"path to serialized db of facial embeddings\")\n",
        "ap.add_argument(\"-r\", \"--recognizer\", required=True,\n",
        "\thelp=\"path to output model trained to recognize faces\")\n",
        "ap.add_argument(\"-l\", \"--le\", required=True,\n",
        "\thelp=\"path to output label encoder\")\n",
        "args = vars(ap.parse_args())"
      ],
      "outputs": [],
      "metadata": {
        "id": "viWFC3NdLUvs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import our packages and modules on Lines 2-5. We’ll be using scikit-learn’s implementation of Support Vector Machines (SVM), a common machine learning model.\n",
        "\n",
        "From there we parse our command line arguments:\n",
        "\n",
        "- --embeddings : The path to the serialized embeddings (we exported it by running the previous extract_embeddings.py script).\n",
        "- --recognizer : This will be our output model that recognizes faces. It is based on SVM. We’ll be saving it so we can use it in the next two recognition scripts.\n",
        "- --le : Our label encoder output file path. We’ll serialize our label encoder to disk so that we can use it and the recognizer model in our image/video face recognition scripts.\n",
        "\n",
        "Each of these arguments is required.\n",
        "\n",
        "Let’s load our facial embeddings and encode our labels:\n"
      ],
      "metadata": {
        "id": "syFi4boZLUvt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# load the face embeddings\n",
        "print(\"[INFO] loading face embeddings...\")\n",
        "data = pickle.loads(open(args[\"embeddings\"], \"rb\").read())\n",
        "# encode the labels\n",
        "print(\"[INFO] encoding labels...\")\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(data[\"names\"])"
      ],
      "outputs": [],
      "metadata": {
        "id": "bqYkUEitLUvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we load our embeddings from Step #1 on Line 19. We won’t be generating any embeddings in this model training script — we’ll use the embeddings previously generated and serialized.\n",
        "\n",
        "Then we initialize our scikit-learn LabelEncoder and encode our name labels (Lines 6-7).\n",
        "\n",
        "Now it’s time to train our SVM model for recognizing faces:\n"
      ],
      "metadata": {
        "id": "r--EW-U2LUvt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# train the model used to accept the 128-d embeddings of the face and\n",
        "# then produce the actual face recognition\n",
        "print(\"[INFO] training model...\")\n",
        "recognizer = SVC(C=1.0, kernel=\"linear\", probability=True)\n",
        "recognizer.fit(data[\"embeddings\"], labels)"
      ],
      "outputs": [],
      "metadata": {
        "id": "NNUcnC5jLUvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On Line 4 we initialize our SVM model, and on Line 5 we fit the model (also known as “training the model”).\n",
        "\n",
        "Here we are using a Linear Support Vector Machine (SVM) but you can try experimenting with other machine learning models if you so wish.\n",
        "\n",
        "After training the model we output the model and label encoder to disk as pickle files.\n"
      ],
      "metadata": {
        "id": "fR6zlYWvLUvu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# write the actual face recognition model to disk\n",
        "f = open(args[\"recognizer\"], \"wb\")\n",
        "f.write(pickle.dumps(recognizer))\n",
        "f.close()\n",
        "# write the label encoder to disk\n",
        "f = open(args[\"le\"], \"wb\")\n",
        "f.write(pickle.dumps(le))\n",
        "f.close()"
      ],
      "outputs": [],
      "metadata": {
        "id": "ei9yk1swLUvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We write two pickle files to disk in this block — the face recognizer model and the label encoder.\n",
        "\n",
        "At this point, be sure you executed the code from Step #1 first.\n",
        "\n",
        "Now that we have finished coding train_model.py as well, let’s apply it to our extracted face embeddings:\n",
        "\n",
        "```shell\n",
        "python train_model.py --embeddings output/embeddings.pickle --recognizer output/recognizer.pickle --le output/le.pickle\n",
        "```\n",
        "\n",
        "Here you can see that our SVM has been trained on the embeddings and both the (1) SVM itself and (2) the label encoding have been written to disk, enabling us to apply them to input images and video."
      ],
      "metadata": {
        "id": "oYPY-j6PLUvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step #3: Recognize faces with OpenCV\n",
        "\n",
        "We are now ready to perform face recognition with OpenCV!\n",
        "\n",
        "We’ll start with recognizing faces in images in this section and then move on to recognizing faces in video streams in the following section.\n",
        "\n",
        "Open up the recognize.py file in your project and insert the following code:\n"
      ],
      "metadata": {
        "id": "X1yXM87-LUvv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# import the necessary packages\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import pickle\n",
        "import cv2\n",
        "import os\n",
        "# construct the argument parser and parse the arguments\n",
        "ap = argparse.ArgumentParser()\n",
        "ap.add_argument(\"-i\", \"--image\", required=True,\n",
        "\thelp=\"path to input image\")\n",
        "ap.add_argument(\"-d\", \"--detector\", required=True,\n",
        "\thelp=\"path to OpenCV's deep learning face detector\")\n",
        "ap.add_argument(\"-m\", \"--embedding-model\", required=True,\n",
        "\thelp=\"path to OpenCV's deep learning face embedding model\")\n",
        "ap.add_argument(\"-r\", \"--recognizer\", required=True,\n",
        "\thelp=\"path to model trained to recognize faces\")\n",
        "ap.add_argument(\"-l\", \"--le\", required=True,\n",
        "\thelp=\"path to label encoder\")\n",
        "ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n",
        "\thelp=\"minimum probability to filter weak detections\")\n",
        "args = vars(ap.parse_args())"
      ],
      "outputs": [],
      "metadata": {
        "id": "wKbU8UITLUvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import our required packages on Lines 2-7. At this point, you should have each of these packages installed.\n",
        "\n",
        "Our six command line arguments are parsed on Lines 9-22:\n",
        "\n",
        "- --image : The path to the input image. We will attempt to recognize the faces in this image.\n",
        "- --detector : The path to OpenCV’s deep learning face detector. We’ll use this model to detect where in the image the face ROIs are.\n",
        "- --embedding-model : The path to OpenCV’s deep learning face embedding model. We’ll use this model to extract the 128-D face embedding from the face ROI — we’ll feed the data into the recognizer.\n",
        "- --recognizer : The path to our recognizer model. We trained our SVM recognizer in Step #2. This is what will actually determine who a face is.\n",
        "- --le : The path to our label encoder. This contains our face labels such as 'raka' or 'anggie' .\n",
        "- --confidence : The optional threshold to filter weak face detections.\n",
        "\n",
        "Be sure to study these command line arguments — it is important to know the difference between the two deep learning models and the SVM model. If you find yourself confused later in this script, you should refer back to here.\n",
        "\n",
        "Now that we’ve handled our imports and command line arguments, let’s load the three models from disk into memory:\n"
      ],
      "metadata": {
        "id": "QEw8aA9_LUvw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# load our serialized face detector from disk\n",
        "print(\"[INFO] loading face detector...\")\n",
        "protoPath = os.path.sep.join([args[\"detector\"], \"deploy.prototxt\"])\n",
        "modelPath = os.path.sep.join([args[\"detector\"],\n",
        "\t\"res10_300x300_ssd_iter_140000.caffemodel\"])\n",
        "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
        "# load our serialized face embedding model from disk\n",
        "print(\"[INFO] loading face recognizer...\")\n",
        "embedder = cv2.dnn.readNetFromTorch(args[\"embedding_model\"])\n",
        "# load the actual face recognition model along with the label encoder\n",
        "recognizer = pickle.loads(open(args[\"recognizer\"], \"rb\").read())\n",
        "le = pickle.loads(open(args[\"le\"], \"rb\").read())"
      ],
      "outputs": [],
      "metadata": {
        "id": "0IL_2dtKLUvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load three models in this block. At the risk of being redundant, I want to explicitly remind you of the differences among the models:\n",
        "\n",
        "- detector : A pre-trained Caffe DL model to detect where in the image the faces are (Lines 3-6).\n",
        "- embedder : A pre-trained Torch DL model to calculate our 128-D face embeddings (Line 9).\n",
        "- recognizer : Our Linear SVM face recognition model (Line 11). We trained this model in Step 2.\n",
        "\n",
        "We also load our label encoder which holds the names of the people our model can recognize (Line 12).\n",
        "\n",
        "Now let’s load our image and detect faces:\n"
      ],
      "metadata": {
        "id": "HK2AuaH8LUvw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# load the image, resize it to have a width of 600 pixels (while\n",
        "# maintaining the aspect ratio), and then grab the image dimensions\n",
        "image = cv2.imread(args[\"image\"])\n",
        "image = imutils.resize(image, width=600)\n",
        "(h, w) = image.shape[:2]\n",
        "# construct a blob from the image\n",
        "imageBlob = cv2.dnn.blobFromImage(\n",
        "\tcv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
        "\t(104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
        "# apply OpenCV's deep learning-based face detector to localize\n",
        "# faces in the input image\n",
        "detector.setInput(imageBlob)\n",
        "detections = detector.forward()"
      ],
      "outputs": [],
      "metadata": {
        "id": "3Yf2aOElLUvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we:\n",
        "\n",
        "- Load the image into memory and construct a blob (Lines 3-9).\n",
        "- Localize faces in the image via our detector (Lines 12-13).\n",
        "\n",
        "Given our new detections , let’s recognize faces in the image. But first we need to filter weak detections and extract the face ROI:\n"
      ],
      "metadata": {
        "id": "dbXOx0ArLUvx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# loop over the detections\n",
        "for i in range(0, detections.shape[2]):\n",
        "\t# extract the confidence (i.e., probability) associated with the\n",
        "\t# prediction\n",
        "\tconfidence = detections[0, 0, i, 2]\n",
        "\t# filter out weak detections\n",
        "\tif confidence > args[\"confidence\"]:\n",
        "\t\t# compute the (x, y)-coordinates of the bounding box for the\n",
        "\t\t# face\n",
        "\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\t\t# extract the face ROI\n",
        "\t\tface = image[startY:endY, startX:endX]\n",
        "\t\t(fH, fW) = face.shape[:2]\n",
        "\t\t# ensure the face width and height are sufficiently large\n",
        "\t\tif fW < 20 or fH < 20:\n",
        "\t\t\tcontinue"
      ],
      "outputs": [],
      "metadata": {
        "id": "vMaESU5xLUvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You’ll recognize this block from Step #1. I’ll explain it here once more:\n",
        "\n",
        "- We loop over the detections on Line 2 and extract the confidence of each on Line 5.\n",
        "- Then we compare the confidence to the minimum probability detection threshold contained in our command line args dictionary, ensuring that the computed probability is larger than the minimum probability (Line 7).\n",
        "- From there, we extract the face ROI (Lines 10-13) as well as ensure it’s spatial dimensions are sufficiently large (Lines 16-17).\n",
        "\n",
        "Recognizing the name of the face ROI requires just a few steps:\n"
      ],
      "metadata": {
        "id": "aG-J3IPqLUvy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\t\t# construct a blob for the face ROI, then pass the blob\n",
        "\t\t# through our face embedding model to obtain the 128-d\n",
        "\t\t# quantification of the face\n",
        "\t\tfaceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255, (96, 96),\n",
        "\t\t\t(0, 0, 0), swapRB=True, crop=False)\n",
        "\t\tembedder.setInput(faceBlob)\n",
        "\t\tvec = embedder.forward()\n",
        "\t\t# perform classification to recognize the face\n",
        "\t\tpreds = recognizer.predict_proba(vec)[0]\n",
        "\t\tj = np.argmax(preds)\n",
        "\t\tproba = preds[j]\n",
        "\t\tname = le.classes_[j]"
      ],
      "outputs": [],
      "metadata": {
        "id": "uCgq3j1RLUvz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we construct a faceBlob (from the face ROI) and pass it through the embedder to generate a 128-D vector which describes the face (Lines 4-7)\n",
        "\n",
        "Then, we pass the vec through our SVM recognizer model (Line 9), the result of which is our predictions for who is in the face ROI.\n",
        "\n",
        "We take the highest probability index (Line 10) and query our label encoder to find the name (Line 12). In between, I extract the probability on Line 11.\n",
        "\n",
        "Note: You cam further filter out weak face recognitions by applying an additional threshold test on the probability. For example, inserting if proba < T (where T is a variable you define) can provide an additional layer of filtering to ensure there are less false-positive face recognitions.\n",
        "\n",
        "Now, let’s display OpenCV face recognition results:\n"
      ],
      "metadata": {
        "id": "KFk9NA-pLUvz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\t\t# draw the bounding box of the face along with the associated\n",
        "\t\t# probability\n",
        "\t\ttext = \"{}: {:.2f}%\".format(name, proba * 100)\n",
        "\t\ty = startY - 10 if startY - 10 > 10 else startY + 10\n",
        "\t\tcv2.rectangle(image, (startX, startY), (endX, endY),\n",
        "\t\t\t(0, 0, 255), 2)\n",
        "\t\tcv2.putText(image, text, (startX, y),\n",
        "\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
        "# show the output image\n",
        "cv2.imshow(\"Image\", image)\n",
        "cv2.waitKey(0)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Mdy_MnX6LUv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For every face we recognize in the loop (including the “unknown”) people:\n",
        "\n",
        "- We construct a text string containing the name and probability on Line 3.\n",
        "- And then we draw a rectangle around the face and place the text above the box (Lines 4-8).\n",
        "\n",
        "And then finally we visualize the results on the screen until a key is pressed (Lines 10 and 11).\n",
        "\n",
        "It is time to recognize faces in images with OpenCV!\n",
        "\n",
        "To apply our OpenCV face recognition pipeline to my provided images (or your own dataset + test images).\n",
        "\n",
        "From there, open up a terminal and execute the following command:\n",
        "\n",
        "```shell\n",
        "python recognize.py --detector face_recognition_model --embedding-model openface.nn4.small2.v1.t7 --recognizer output/recognizer.pickle --le output/le.pickle --image images/raka.png\n",
        "```\n"
      ],
      "metadata": {
        "id": "WpCtzkgGLUv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recognize faces in video streams\n",
        "\n",
        "Open up the recognize_video.py file and let’s get started:\n"
      ],
      "metadata": {
        "id": "0HMAES8qLUv1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# import the necessary packages\n",
        "from imutils.video import VideoStream\n",
        "from imutils.video import FPS\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import pickle\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        "# construct the argument parser and parse the arguments\n",
        "ap = argparse.ArgumentParser()\n",
        "ap.add_argument(\"-d\", \"--detector\", required=True,\n",
        "\thelp=\"path to OpenCV's deep learning face detector\")\n",
        "ap.add_argument(\"-m\", \"--embedding-model\", required=True,\n",
        "\thelp=\"path to OpenCV's deep learning face embedding model\")\n",
        "ap.add_argument(\"-r\", \"--recognizer\", required=True,\n",
        "\thelp=\"path to model trained to recognize faces\")\n",
        "ap.add_argument(\"-l\", \"--le\", required=True,\n",
        "\thelp=\"path to label encoder\")\n",
        "ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n",
        "\thelp=\"minimum probability to filter weak detections\")\n",
        "args = vars(ap.parse_args())"
      ],
      "outputs": [],
      "metadata": {
        "id": "gR4MtiAFLUv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our imports are the same as the Step #3 section above, except for Lines 2 and 3 where we use the imutils.video module. We’ll use VideoStream to capture frames from our camera and FPS to calculate frames per second statistics.\n",
        "\n",
        "The command line arguments are also the same except we aren’t passing a path to a static image via the command line. Rather, we’ll grab a reference to our webcam and then process the video. Refer to Step #3 if you need to review the arguments.\n",
        "\n",
        "Our three models and label encoder are loaded here:"
      ],
      "metadata": {
        "id": "z3EgCgfuLUv2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# load our serialized face detector from disk\n",
        "print(\"[INFO] loading face detector...\")\n",
        "protoPath = os.path.sep.join([args[\"detector\"], \"deploy.prototxt\"])\n",
        "modelPath = os.path.sep.join([args[\"detector\"],\n",
        "\t\"res10_300x300_ssd_iter_140000.caffemodel\"])\n",
        "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
        "# load our serialized face embedding model from disk\n",
        "print(\"[INFO] loading face recognizer...\")\n",
        "embedder = cv2.dnn.readNetFromTorch(args[\"embedding_model\"])\n",
        "# load the actual face recognition model along with the label encoder\n",
        "recognizer = pickle.loads(open(args[\"recognizer\"], \"rb\").read())\n",
        "le = pickle.loads(open(args[\"le\"], \"rb\").read())"
      ],
      "outputs": [],
      "metadata": {
        "id": "8zmSfrBPLUv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we load face detector , face embedder model, face recognizer model (Linear SVM), and label encoder.\n",
        "\n",
        "Again, be sure to refer to Step #3 if you are confused about the three models or label encoder.\n",
        "\n",
        "Let’s initialize our video stream and begin processing frames:"
      ],
      "metadata": {
        "id": "JrF_-WziLUv3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# initialize the video stream, then allow the camera sensor to warm up\n",
        "print(\"[INFO] starting video stream...\")\n",
        "vs = VideoStream(src=0).start()\n",
        "time.sleep(2.0)\n",
        "# start the FPS throughput estimator\n",
        "fps = FPS().start()\n",
        "# loop over frames from the video file stream\n",
        "while True:\n",
        "\t# grab the frame from the threaded video stream\n",
        "\tframe = vs.read()\n",
        "\t# resize the frame to have a width of 600 pixels (while\n",
        "\t# maintaining the aspect ratio), and then grab the image\n",
        "\t# dimensions\n",
        "\tframe = imutils.resize(frame, width=600)\n",
        "\t(h, w) = frame.shape[:2]\n",
        "\t# construct a blob from the image\n",
        "\timageBlob = cv2.dnn.blobFromImage(\n",
        "\t\tcv2.resize(frame, (300, 300)), 1.0, (300, 300),\n",
        "\t\t(104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
        "\t# apply OpenCV's deep learning-based face detector to localize\n",
        "\t# faces in the input image\n",
        "\tdetector.setInput(imageBlob)\n",
        "\tdetections = detector.forward()"
      ],
      "outputs": [],
      "metadata": {
        "id": "zXygua3kLUv4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our VideoStream object is initialized and started on Line 3. We wait for the camera sensor to warm up on Line 4.\n",
        "\n",
        "We also initialize our frames per second counter (Line 6) and begin looping over frames on Line 8. We grab a frame from the webcam on Line 10.\n",
        "\n",
        "From here everything is the same as Step 3. We resize the frame (Line 14) and then we construct a blob from the frame + detect where the faces are (Lines 17-23).\n",
        "\n",
        "Now let’s process the detections:"
      ],
      "metadata": {
        "id": "bB3KvG1XLUv5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\t# loop over the detections\n",
        "\tfor i in range(0, detections.shape[2]):\n",
        "\t\t# extract the confidence (i.e., probability) associated with\n",
        "\t\t# the prediction\n",
        "\t\tconfidence = detections[0, 0, i, 2]\n",
        "\t\t# filter out weak detections\n",
        "\t\tif confidence > args[\"confidence\"]:\n",
        "\t\t\t# compute the (x, y)-coordinates of the bounding box for\n",
        "\t\t\t# the face\n",
        "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\t\t\t# extract the face ROI\n",
        "\t\t\tface = frame[startY:endY, startX:endX]\n",
        "\t\t\t(fH, fW) = face.shape[:2]\n",
        "\t\t\t# ensure the face width and height are sufficiently large\n",
        "\t\t\tif fW < 20 or fH < 20:\n",
        "\t\t\t\tcontinue"
      ],
      "outputs": [],
      "metadata": {
        "id": "JxUR6kyJLUv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it’s time to perform OpenCV face recognition:"
      ],
      "metadata": {
        "id": "5JGNeLwxLUv5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\t\t\t# construct a blob for the face ROI, then pass the blob\n",
        "\t\t\t# through our face embedding model to obtain the 128-d\n",
        "\t\t\t# quantification of the face\n",
        "\t\t\tfaceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
        "\t\t\t\t(96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
        "\t\t\tembedder.setInput(faceBlob)\n",
        "\t\t\tvec = embedder.forward()\n",
        "\t\t\t# perform classification to recognize the face\n",
        "\t\t\tpreds = recognizer.predict_proba(vec)[0]\n",
        "\t\t\tj = np.argmax(preds)\n",
        "\t\t\tproba = preds[j]\n",
        "\t\t\tname = le.classes_[j]\n",
        "\t\t\t# draw the bounding box of the face along with the\n",
        "\t\t\t# associated probability\n",
        "\t\t\ttext = \"{}: {:.2f}%\".format(name, proba * 100)\n",
        "\t\t\ty = startY - 10 if startY - 10 > 10 else startY + 10\n",
        "\t\t\tcv2.rectangle(frame, (startX, startY), (endX, endY),\n",
        "\t\t\t\t(0, 0, 255), 2)\n",
        "\t\t\tcv2.putText(frame, text, (startX, y),\n",
        "\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
        "\t# update the FPS counter\n",
        "\tfps.update()"
      ],
      "outputs": [],
      "metadata": {
        "id": "N9Rui2q4LUv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s display the results and clean up:"
      ],
      "metadata": {
        "id": "SPrun6WALUv6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\t# show the output frame\n",
        "\tcv2.imshow(\"Frame\", frame)\n",
        "\tkey = cv2.waitKey(1) & 0xFF\n",
        "\t# if the `q` key was pressed, break from the loop\n",
        "\tif key == ord(\"q\"):\n",
        "\t\tbreak\n",
        "# stop the timer and display FPS information\n",
        "fps.stop()\n",
        "print(\"[INFO] elasped time: {:.2f}\".format(fps.elapsed()))\n",
        "print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
        "# do a bit of cleanup\n",
        "cv2.destroyAllWindows()\n",
        "vs.stop()"
      ],
      "outputs": [],
      "metadata": {
        "id": "QGF1Tr1sLUv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To execute our OpenCV face recognition pipeline on a video stream, open up a terminal and execute the following command:\n",
        "\n",
        "```shell\n",
        "python recognize_video.py --detector face_recognition_model --embedding-model openface.nn4.small2.v1.t7 --recognizer output/recognizer.pickle --le output/le.pickle\n",
        "```\n"
      ],
      "metadata": {
        "id": "fKw1BIyOLUv6"
      }
    }
  ],
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit"
    },
    "interpreter": {
      "hash": "60716cfbf8f6257f052921a35c79929cd52e6aa52b65b24e32100e25d3fdc1e3"
    },
    "colab": {
      "name": "d1pm_opencv.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}