# -*- coding: utf-8 -*-
"""processing_sequences_using_rnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QXHVWYnPsMu2_9ac9sH7ag2wnZ7Nc3AF

## Processing Sequences Using RNNs

# Setup

First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0.
"""

# Commented out IPython magic to ensure Python compatibility.
# Python ≥3.5 is required
import sys
assert sys.version_info >= (3, 5)

# Is this notebook running on Colab or Kaggle?
IS_COLAB = "google.colab" in sys.modules
IS_KAGGLE = "kaggle_secrets" in sys.modules

# Scikit-Learn ≥0.20 is required
import sklearn
assert sklearn.__version__ >= "0.20"

# TensorFlow ≥2.0 is required
import tensorflow as tf
from tensorflow import keras
assert tf.__version__ >= "2.0"

if not tf.config.list_physical_devices('GPU'):
    print("No GPU was detected. LSTMs and CNNs can be very slow without a GPU.")
    if IS_COLAB:
        print("Go to Runtime > Change runtime and select a GPU hardware accelerator.")
    if IS_KAGGLE:
        print("Go to Settings > Accelerator and select GPU.")

# Common imports
import numpy as np
import os
from pathlib import Path

# to make this notebook's output stable across runs
np.random.seed(42)
tf.random.set_seed(42)

# To plot pretty figures
# %matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "rnn"
IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID)
os.makedirs(IMAGES_PATH, exist_ok=True)

def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = os.path.join(IMAGES_PATH, fig_id + "." + fig_extension)
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)

"""# A. Basic Recurrent Neural Networks

## Generate the Dataset

Let's generate some data to play with Recurrent Neural Network. We will generate 10,000 data that contains 7,000 train data, 2,000 validation data, and 1,000 test data.
"""

# Function or Generate Data. The Generated Data is Assumed to be Time Series Data
def generate_time_series(batch_size, n_steps):
    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)
    time = np.linspace(0, 1, n_steps)
    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  #   wave 1
    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2
    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)   # + noise
    return series[..., np.newaxis].astype(np.float32)

# Generate Dataset that Each Row/Data Contains 20 Features/Columns/Time Step

np.random.seed(42)

n_steps = 20
series = generate_time_series(10000, n_steps + 1)
X_train, y_train = series[:7000, :n_steps], series[:7000, -1]
X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]
X_test, y_test = series[9000:, :n_steps], series[9000:, -1]

# Display the First Two Data

print('X_train - 0 : ', X_train[0])
print('y_train - 0 : ', y_train[0])
print('')

print('X_train - 1 : ', X_train[1])
print('y_train - 1 : ', y_train[1])
print('')

# Visualization of the First Three Data

def plot_series(series, y=None, y_pred=None, x_label="$t$", y_label="$x(t)$"):
    plt.plot(series, ".-")
    if y is not None:
        plt.plot(n_steps, y, "bx", markersize=10)
    if y_pred is not None:
        plt.plot(n_steps, y_pred, "ro")
    plt.grid(True)
    if x_label:
        plt.xlabel(x_label, fontsize=16)
    if y_label:
        plt.ylabel(y_label, fontsize=16, rotation=0)
    plt.hlines(0, 0, 100, linewidth=1)
    plt.axis([0, n_steps + 1, -1, 1])

fig, axes = plt.subplots(nrows=1, ncols=3, sharey=True, figsize=(12, 4))
for col in range(3):
    plt.sca(axes[col])
    plot_series(X_train[col, :, 0], y_train[col, 0],
                y_label=("$x(t)$" if col==0 else None))
save_fig("time_series_plot")
plt.show()

"""Symbol `X` in graph above indicates the target (`y`).

---

## Computing Some Baselines (Dummy Model)

Before we start using RNNs, it is often a good idea to have a few baseline
metrics/models. Reasons : 
* We may end up thinking our model works great when **in fact it is doing worse** than basic models. 
* Basic models will be our reference in making the model where the model we make **must have a smaller error value** compare to error of basic model.
* In this case, the simplest
approach is to **predict the last value** in each series. This is called naive forecasting, and it is sometimes surprisingly difficult to outperform.
"""

# Compute Last Value as Predictions

y_pred = X_test[:, -1]
mse_last_value = np.mean(keras.losses.mean_squared_error(y_test, y_pred))
print('MSE - Last Value : ', mse_last_value)

# Display Predictions for First Data

print('X_test[0] : ', X_test[0])
print('y_pred[0] : ', y_pred[0])
print('y_test[0] : ', y_test[0])

# Visualization of Predictions for First Data

plot_series(X_test[0, :, 0], y_test[0, 0], y_pred[0, 0])
plt.show()

"""In the graph above, `X` means the target (`y_test`) meanwhile `red-dot` is our prediction (`y_pred`)

---

## Simple Neural Network

Let's solve this problem using Neural Network. This time we will solve it by using only 1 input layer and 1 output layer where the **output neurons are only 1**.
"""

# Commented out IPython magic to ensure Python compatibility.
# # Let's Try Solve the Problem using Simple Neural Network
# 
# %%time
# np.random.seed(42)
# tf.random.set_seed(42)
# 
# model = keras.models.Sequential([
#     keras.layers.Flatten(input_shape=[20, 1]),
#     keras.layers.Dense(1)
# ])
# 
# model.compile(loss="mse", optimizer="adam")
# history = model.fit(X_train, y_train, epochs=100,
#                     validation_data=(X_valid, y_valid))

# Visualization of Training Loss and Validation Loss

def plot_learning_curves(loss, val_loss):
    plt.plot(np.arange(len(loss)) + 0.5, loss, "b.-", label="Training loss")
    plt.plot(np.arange(len(val_loss)) + 1, val_loss, "r.-", label="Validation loss")
    plt.gca().xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))
    plt.axis([1, 20, 0, 0.05])
    plt.legend(fontsize=14)
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.grid(True)

plot_learning_curves(history.history["loss"], history.history["val_loss"])
plt.show()

# Architecture of Our Simple Neural Network

model.summary()

# Compute MSE and Compare to MSE from Naive Forecasting (using Last Value)

mse_simple_nn = model.evaluate(X_test, y_test)

print('MSE - Last Value            : ', mse_last_value)
print('MSE - Simple Neural Network : ', mse_simple_nn)

# Visualization of Predictions for First Data

y_pred = model.predict(X_test)
plot_series(X_test[0, :, 0], y_test[0, 0], y_pred[0, 0])
plt.show()

"""In the graph above, `X` means the target (`y_valid`) meanwhile `red-dot` is our prediction (`y_pred`)

---

## Using a Simple Recurrent Neural Network (RNN)

Let's try solve this problem using simple Recurrent Neural Network. This time we will solve it by using **only 1 RNN's neuron**.
"""

# Commented out IPython magic to ensure Python compatibility.
# # Let's Try Solve the Problem using Simple Recurrent Neural Network
# 
# %%time
# np.random.seed(42)
# tf.random.set_seed(42)
# 
# model = keras.models.Sequential([
#     keras.layers.SimpleRNN(1, input_shape=[20, 1])
# ])
# 
# optimizer = keras.optimizers.Adam(lr=0.005)
# model.compile(loss="mse", optimizer=optimizer)
# history = model.fit(X_train, y_train, epochs=100,
#                     validation_data=(X_valid, y_valid))

# Visualization of Training Loss and Validation Loss

plot_learning_curves(history.history["loss"], history.history["val_loss"])
plt.show()

# Architecture of Our Simple Recurrent Neural Network

model.summary()

# Compute MSE and Compare to MSE from Naive Forecasting (using Last Value) and from Simple Neural Network

mse_simple_rnn = model.evaluate(X_test, y_test)

print('\nMSE - Last Value                      : ', mse_last_value)
print('MSE - Simple Neural Network           : ', mse_simple_nn)
print('MSE - Simple Recurrent Neural Network : ', mse_simple_rnn)

# Visualization of Predictions for First Data

y_pred = model.predict(X_test)
plot_series(X_test[0, :, 0], y_test[0, 0], y_pred[0, 0])
plt.show()

"""In the graph above, `X` means the target (`y_valid`) meanwhile `red-dot` is our prediction (`y_pred`)

---

## Using Deep Recurrent Neural Network

This time we will solve it by using RNN neurons where we will stack multiple layers of RNN.

Note : Make sure to set `return_sequences=True` for all recurrent layers (except the last one, if you only care about the last output). If you don’t, they will output a 2D array (containing only the output of the last time step) instead of a 3D array (containing outputs for all time steps), and the next recurrent layer will complain that you are not feeding it sequences in the expected 3D format.
"""

# Commented out IPython magic to ensure Python compatibility.
# # Let's Try Solve the Problem using Deep Recurrent Neural Network
# 
# %%time
# np.random.seed(42)
# tf.random.set_seed(42)
# 
# model = keras.models.Sequential([
#     keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),
#     keras.layers.SimpleRNN(20, return_sequences=True),
#     keras.layers.SimpleRNN(1)
# ])
# 
# model.compile(loss="mse", optimizer="adam")
# history = model.fit(X_train, y_train, epochs=100,
#                     validation_data=(X_valid, y_valid))

# Visualization of Training Loss and Validation Loss

plot_learning_curves(history.history["loss"], history.history["val_loss"])
plt.show()

# Compute MSE and Compare to MSE from Naive Forecasting (using Last Value), Simple Neural Network, and from Simple Recurrent Neural Network

mse_deep_rnn = model.evaluate(X_test, y_test)

print('\nMSE - Last Value                      : ', mse_last_value)
print('MSE - Simple Neural Network           : ', mse_simple_nn)
print('MSE - Simple Recurrent Neural Network : ', mse_simple_rnn)
print('MSE - Deep Recurrent Neural Network   : ', mse_deep_rnn)

# Visualization of Predictions for First Data

y_pred = model.predict(X_test)
plot_series(X_test[0, :, 0], y_test[0, 0], y_pred[0, 0])
plt.show()

"""---
Let's change last layer to Dense Layer. This is because :
* In real-world application, **it is rare** that output layer is recurrent neurons. 
* Also, since **a SimpleRNN layer uses the tanh activation function** by default, the predicted values must lie within the range –1 to 1. But what if you want to use another activation function? 

For both these reasons, it might be preferable to replace the output layer with a Dense layer: it would run slightly faster, the accuracy would be roughly the same, and it would allow us to choose any output activation function we want. 

If you make this change, also make sure to remove `return_sequences=True` from the second (now last) recurrent layer.
"""

# Commented out IPython magic to ensure Python compatibility.
# # Let's Try Solve the Problem using Deep Recurrent Neural Network with Dense Layer as Last Layer
# 
# %%time
# np.random.seed(42)
# tf.random.set_seed(42)
# 
# model = keras.models.Sequential([
#     keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),
#     keras.layers.SimpleRNN(20),
#     keras.layers.Dense(1)
# ])
# 
# model.compile(loss="mse", optimizer="adam")
# history = model.fit(X_train, y_train, epochs=100,
#                     validation_data=(X_valid, y_valid))

# Visualization of Training Loss and Validation Loss

plot_learning_curves(history.history["loss"], history.history["val_loss"])
plt.show()

# Compute MSE and Compare to MSE from Naive Forecasting (using Last Value), Simple Neural Network, Simple Recurrent Neural Network, and Deep Recurrent Neural Network

mse_deep_rnn_dense = model.evaluate(X_test, y_test)

print('\nMSE - Last Value                                  : ', mse_last_value)
print('MSE - Simple Neural Network                       : ', mse_simple_nn)
print('MSE - Simple Recurrent Neural Network             : ', mse_simple_rnn)
print('MSE - Deep Recurrent Neural Network               : ', mse_deep_rnn)
print('MSE - Deep Recurrent Neural Network - Dense Layer : ', mse_deep_rnn_dense)

# Visualization of Predictions for First Data

y_pred = model.predict(X_test)
plot_series(X_test[0, :, 0], y_test[0, 0], y_pred[0, 0])
plt.show()

"""Notes:

* Although the MSE value of `RNN-Dense` layers has a larger error than `RNN` layers only, this **error is still in approximately the same range**.

* We can see that with `RNN-Dense` layers, the **processing time is faster** (4 minutes 23 seconds) than using `RNN` layers only (5 minutes 24 seconds).

# B. LSTMs
"""

# Commented out IPython magic to ensure Python compatibility.
# # Let's Try Solve the Problem using LSTM
# 
# %%time
# np.random.seed(42)
# tf.random.set_seed(42)
# 
# model = keras.models.Sequential([
#     keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),
#     keras.layers.LSTM(20),
#     keras.layers.Dense(1)
# ])
# 
# model.compile(loss="mse", optimizer="adam")
# history = model.fit(X_train, y_train, epochs=100,
#                     validation_data=(X_valid, y_valid))

# Visualization of Training Loss and Validation Loss

plot_learning_curves(history.history["loss"], history.history["val_loss"])
plt.show()

# Compute MSE and Compare to MSE from Other Algorithms

mse_lstm = model.evaluate(X_test, y_test)

print('\nMSE - Last Value                                  : ', mse_last_value)
print('MSE - Simple Neural Network                       : ', mse_simple_nn)
print('MSE - Simple Recurrent Neural Network             : ', mse_simple_rnn)
print('MSE - Deep Recurrent Neural Network               : ', mse_deep_rnn)
print('MSE - Deep Recurrent Neural Network - Dense Layer : ', mse_deep_rnn_dense)
print('MSE - LSTM                                        : ', mse_lstm)

"""# C. GRUs"""

# Let's Try Solve the Problem using GRU

np.random.seed(42)
tf.random.set_seed(42)

model = keras.models.Sequential([
    keras.layers.GRU(20, return_sequences=True, input_shape=[None, 1]),
    keras.layers.GRU(20),
    keras.layers.Dense(1)
])

model.compile(loss="mse", optimizer="adam")
history = model.fit(X_train, y_train, epochs=100,
                    validation_data=(X_valid, y_valid))

# Visualization of Training Loss and Validation Loss

plot_learning_curves(history.history["loss"], history.history["val_loss"])
plt.show()

# Compute MSE and Compare to MSE from Other Algorithms

mse_gru = model.evaluate(X_test, y_test)

print('\nMSE - Last Value                                  : ', mse_last_value)
print('MSE - Simple Neural Network                       : ', mse_simple_nn)
print('MSE - Simple Recurrent Neural Network             : ', mse_simple_rnn)
print('MSE - Deep Recurrent Neural Network               : ', mse_deep_rnn)
print('MSE - Deep Recurrent Neural Network - Dense Layer : ', mse_deep_rnn_dense)
print('MSE - LSTM                                        : ', mse_lstm)
print('MSE - GRU                                         : ', mse_gru)

# Visualization of Predictions for First Data

y_pred = model.predict(X_test)
plot_series(X_test[0, :, 0], y_test[0, 0], y_pred[0, 0])
plt.show()